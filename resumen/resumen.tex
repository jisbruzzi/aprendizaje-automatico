\documentclass[12pt, a4paper]{article}
\usepackage[utf8x]{inputenc}
\usepackage[spanish]{babel}
\usepackage{pdfpages}
\usepackage[T1]{fontenc} %Me deja combinar la negrita con las Mayusculas

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{dsfont} %para el 1
\usepackage{mathrsfs}
\usepackage{enumitem} %Para la enumeracion con letras

\usepackage{graphicx} %para la imagen
\usepackage{subcaption} %para poner varias imagenes juntas
\usepackage{float}


\usepackage{chngcntr} %Para resetear el contador de las ecuaciones por secciones y subsecciones
\counterwithin*{equation}{section}
\counterwithin*{equation}{subsection}

\title{Tarea de Aprendizaje Estadístico}
\author{}
\date{}

\begin{document}
\begin{titlepage} %Inicio de la caratula del tp
	\centering
	  \includegraphics[width=0.15\textwidth]{FIUBA_logo}\par
	  {\scshape\Large Universidad de Buenos Aires
      \\ Facultad de Ingenieria \par}
      {\scshape\small Año 2018 - 2er Cuatrimestre \par}
	  \vspace{1cm}
	  {\scshape\bfseries\LARGE Aprendizaje Estadístico, Teoría y aplicación\par}
	  \vspace{0.5cm}
	  \vspace{1cm}
      {\scshape\large Resúmen de la materia y devolutiva \par}
      \vspace{0.5cm
      \raggedright}
      \vspace{0.5cm}
    \centering
	  {\normalsize Sbruzzi, José Ignacio - Ingeniería Informática \#97452 \par}
      {\small  jose.sbru@gmail.com \par}
\end{titlepage} %Cerrado de la caratula del tp
\newpage
\tableofcontents
\newpage
\section{Clase 1 (31/8)}
La primera parte de esta clase fue un repaso de diversos temas que serán útiles durante la cursada:
\begin{itemize}
    \item Teorema de pitágoras
    \item espacios euclídeos
    \item Ortogonalidad
    \item Relación entre el producto interno y el coseno
    \item Desigualdad Cauchy-Schwartz
    \item Norma inducida
    \item Proyección Ortogonal
    \item Definición de esperanza
    \item El espacio algebraico de variables aleatorias
    \item Desigualdad de Markov
    \item Desigualdad de Chebyshev
    \item Desigualdad de Chernoff
    \item Desigualdad de Jensen
    \item Función convexa
    \item Esperanza condicional
\end{itemize}
La segunda parte de la clase se habló del problema de la comunicación digital para ilustrar la lógica por detrás de la construcción de un clasificador bayesiano.
Siendo $\delta(r)$ una función que predice el dígito (0 o 1) emitido a partir del recibido $r \in \{0,1\}$. $P(S=s|R=r)$ es la probabilidad de que se haya emitido el dígito $s$ dado que se recibió el dígito $r$.
$$\delta(r)=\mathds{1}\{\mathds{P}(S=1|R=r) > \mathds{P}(S=0|R=r)\}$$
Así, este clasificador toma la mejor decisión posible para la información que se tiene disponible ($r$), con lo cual es un clasificador bayesiano.

\section{Clase 2 (7/9)}
\subsection{Definiciones iniciales}
\subsubsection{Clasificador}
$$g:\mathds{R}^d\rightarrow\{ 1,2,...,M \}$$
$g(x)$ representa una conjetura respecto de la naturaleza de la distribución de las $x$. El clasificador se equivoca cuando $g(x) \neq y$.
\subsubsection{Calidad del clasificador}
Sea $(X,Y) \in \mathds{R}^d \times \{1,2, ..., M \}$ un par donde $X$ es una variable aleatoria que representa las propiedades observables y $Y$ la característica a predecir. Así, se define la pérdida de un clasificador como 
$L(g) = \mathds{P} ( g(X) \neq Y )$.
\subsubsection{Clasificador bayesiano}
Es el mejor clasificador, definido por
$$ argmin_{g:\mathds{R}^d \rightarrow \{1, ..., M\}}\{\mathds{P}( g(X) \neq Y )\} = g^{*}$$

$$L^{*}=L(g^{*})$$

No se da siempre que $L^{*}=0$ porque $Y$ podría no ser una función de $X$.

\subsubsection{Dataset de entrenamiento}
Se denota como $(X_i,Y_i), i = 1, 2, ..., n$; donde las parejas $(X_i,Y_i)$ son observaciones independientes e identicamente distribuidas, al igual que $(X,Y)$.

$$ D_n = \{(X_i,Y_i), i = 1, 2, ..., n\} $$

Así, en realidad, cuando aplicamos algoritmos de machine learning tenemos una g denotada como:
$$g(X,(X_1,Y_1), (X_2,Y_2), ..., (X_n,Y_n))$$
Donde $X$ es una nueva observación.

Es decir,
$$g_n:\mathds{R}^d \times ( \mathds{R}^d \times \{ 1, ..., M \})^n \rightarrow \{1,...,M\}$$

Así, tenemos $$L_n=L(g_n) = \mathds{P} ( g(X,(X_1,Y_1), ..., (X_n,Y_n)) \neq Y | (X_1,Y_1), ..., (X_n,Y_n))$$

Con lo cual $L_n$ es una variable aleatoria dependiente de las observaciones.
\subsection{Clasificador bayesiano para M=2}

Sean (con $A \subset \mathds{R}^d$, $x \in \mathds{R}^d$ , $y\in \{0,1\}$):

$$\mu(A)=\mathds{P}(x \in A)$$
$$\eta(x)=\mathds{P}(Y=1 | X=x)=\mathds{E}[Y|X=x]$$

Así, $$\eta(x)=\int_{C}\mathds{P}(Y=0|X=x) \mu(dx) + \int_{C}\mathds{P}(Y=1|X=x) \mu(dx)$$
Siendo $C=\mathds{R}^d \times \{0,1\}$.

Bajo estas condiciones, $$g^{*}(x)=\mathds{1}\{ \eta(x)>\frac{1}{2} \}$$

\section{Clase 3 (14/9)}

\subsection{Plug-in decision}

Una "plug-in decision" (decisión "enchufada") es una función $g$ definida por medio de una cierta función $\tilde{\eta}(x)$. Así, la función de decisión plug-in se define como:

$$g(x)=\mathds{1}\{ \tilde{\eta}(x) > \frac{1}{2}\}$$

En clase se demostró un teorema que establece que 

$$L(g) - L^{*}(g) \leq \int_{\mathds{R}^d} |\eta(x)-\tilde{\eta}(x)| \mu(dx) = 2 \mathds{E}[ \eta(X) - \tilde { \eta}(X) ]$$

Es decir, que si las funciones $\eta(x)$ y $\tilde{\eta}(x)$ son funciones similares (lo cual se ve más claramente en el miembro central de la fórmula anterior), los errores cometidos también serán similares. Es decir que, cuanto más se parezca $\eta$ a $\tilde{\eta}$, más cerca estará el error de $g$ del menor error posible (que es el de $g^{*}$).
\subsection{Convergencia debil y fuerte}
Una regla de clasificación $g_n$ es consistente si, para ciertas distribuciones de $(X,Y)$, se cumple:

$$\mathds{E}[L_n] = \mathds{P}(g_n(X,D_n)\neq Y) \rightarrow L^{*}\text{ cuando } n \rightarrow \infty$$

Y es fuertamente consistente si 

$$\mathop{lim}_{n \rightarrow \infty} L_n = L^{*} \text{ con probabilidad 1}$$

Una regla de clasificación es \textbf{universalmente consistente} si es fuertemente consistente para cualquier distribución de $(X,Y)$.

\subsection{Reglas basadas en particiones}
Muchas reglas de clasificación particionan el espacio en celdas disjuntas $A_i$, de forma que $$\mathds{R}^d = \mathop{\bigcup}_{i=1}^{\infty}A_i$$
La regla se basa en la "mayoría electoral", es decir, si $x$ pertenece a cierto $A_i$, entonces $g$ le asignará el valor más común de $y_i$ para los $x_i$ pertenecientes a $A_i$. Es decir,

$$g_n(x)=\mathds{1}\bigg \{ \mathop{\sum}_{i=1}^{n} \mathds{1}\{Y_i=1\} \mathds{1}\{X_i \in A(x)\} \geq \mathop{\sum}_{i=1}^{n} \mathds{1}\{Y_i=0\} \mathds{1}\{X_i \in A(x)\} \bigg \}$$

donde $A(x)$ es el $A_i$ al que pertenece $x$.
Sea el diámetro de un conjunto contenido en $\mathds{R}^d$ definido como:
$$diam(A)=\mathop{sup}_{x,y \in A} || x-y||$$

Y sea la cantidad de $X_i$ presentes en la misma celda que $x$ definida como:
$$N(x)=\mathop{\sum}_{i=1}^{n}\mathds{1}\{ X_i \in A(x) \}$$

La regla $g_n$ definida más arriba es consistente cuando se cumplen las siguientes condiciones:
$$diam(A(X))\rightarrow 0 \text{ en probabilidad}$$
$$N(X)\rightarrow \infty \text{ en probabilidad}$$

Es decir, los $A_i$ deben ser tales que su tamaño decrece a medida que crece $n$ pero la cantidad de puntos que contiene crece junto con $n$: deben ir reduciendo su tamaño pero no demasiado rápido, no deben tender a "vaciarse".

\subsection{La regla del histograma}
La regla del histograma es un caso especial de la regla de clasificación de la sección anterior en la que los $A_i$ son hipercubos de dimensión $d$ y de lado $h_n$.

Esta regla es universalmente consistente si se cumplen las siguientes condiciones:
$$h_n \rightarrow 0 \text{ cuando } n\rightarrow \infty$$
$$nh_n^d \rightarrow \infty \text{ cuando } n\rightarrow \infty$$

Estas condiciones son análogas a las de la sección anterior, con la diferencia de que cuando el espacio se parte en hipercubos se obtiene consistencia universal.

\section{Clase 4 (28/9)}
\subsection{El teorema de Stone}
El teorema de Stone indica condiciones bajo las cuales un clasificador que podría verse como una generalización de los de la clase 3 converge universalmente.

Se definen:
$$ \eta_n(x)=\mathop{\sum}_{i=1}^{n} \mathds{1}\{Y_i=1\} W_{ni}(x) $$
Siendo:
$$ \sum_{i=1}^n W_{ni}(x)=1 $$
Y se define la regla de clasificación como:
$$ g_n(x) = \mathds{1}\bigg \{ \sum_{i=1}^n \mathds{1}\{Y_i=1\} W_{ni}(x) \geq \mathds{1}\{Y_i=0\} W_{ni}(x) \bigg \} $$

$g_n$ converge universalmente cuando se cumplen las siguientes tres condiciones:

\subsubsection{Condición 1}
Existe una constante $c$ tal que, para cualquier función medible $f$ tal que $\mathds{E}[f(X)]<\infty$,
$$\mathds{E} \bigg \{ \sum_{i=1}^n W_{ni}(X)f(X_i) \bigg \} \leq c \mathds{E}[f(X)]$$

\subsubsection{Condición 2}
Para todo $a >0$,

$$\mathop{lim}_{ n \rightarrow \infty} \mathds{E} \bigg \{ \sum_{i=1}^n W_{ni}(X)\mathds{1}\{ || X_i -X ||>a \} \bigg \} = 0$$

\subsubsection{Condición 3}

$$ \mathop{lim}_{n \rightarrow \infty} \mathds{E} \bigg \{  \mathop{max}_{1 \leq i \leq n} W_{ni}(X)\bigg \} $$

\section{Clase 5 (5/10)}
\subsection{Desigualdad de Hoeffding}
Sean $X_1, ..., X_n$ variables aleatoreas independientes y sean $a_i$ y $b_i$ tales que $\mathds{P}(a_i \leq X_i \leq b_i)=1$, y sea $S_n=\sum_{i=1}^n X_i$. Entonces, para cualquier $\epsilon > 0$ se cumplen:
$$\mathds{P}(S_n - \mathds{E}[S_n] \geq \epsilon) \leq e^{-2 \epsilon^2 / \sum_{i=1}^n (b_i-a_i)^2}$$
y también
$$\mathds{P}(S_n - \mathds{E}[S_n] \leq - \epsilon) \leq e^{-2 \epsilon^2 / \sum_{i=1}^n (b_i-a_i)^2}$$

\subsection{Cómo estimar L}

Sea el estimador de $L$: $$\widehat{L}_{n,m}=\frac{1}{m} \sum_{j=1}^m \mathds{1} \bigg \{ g_n(X_{n+i}) \neq Y_{n+i} \bigg \}$$

$\widehat{L}_{n,m}$ es un estimador insesgado, es decir, $\mathds{E}[\widehat{L}_{n,m}|D_n]=L_n$

Aplicando la desigualdad de Hoeffding podemos deducir que:

$$ \mathds{P}(|\widehat{L}_{n,m}-L_n|>\epsilon | D_n) \leq 2e^{-2m\epsilon^2} $$

Es decir, independientemente de la distribución de $(X,Y)$ se puede acotar el error que tiene el estimador de $L_n$.
\subsection{Cómo elegir clasificadores}

$C$ es una família de funciones $\phi:\mathds(R)^d \rightarrow \{0,1\}$

$\phi^{*}_n$ es el clasificador de $C$ que minimiza la $L_n$ estimada: $$ \widehat{L_n}(\phi^{*}_n) \leq \widehat{L_n}(\phi) \text{ para todo } \phi \in C$$

Así, según descubierto por Vapnik y Chervonenkis:

$$L(\phi^{*}_n)-\mathop{inf}_{\phi \in C}L(\phi) \leq 2 \mathop{sup}_{\phi \in C} |\widehat{L_n}(\phi) - L(\phi)|$$

$$|\widehat{L_n}(\phi^{*}_n) - L(\phi^{*}_n)| \leq \mathop{sup}_{\phi \in C} |\widehat{L_n}(\phi) - L(\phi)|$$

Además, si $|C| \leq N$, tenemos que, para todo $\epsilon > 0$,

$$ \mathds{P} \bigg \{ \mathop{sup}_{\phi \in C} |\widehat{L_n}(\phi) - L(\phi)|>\epsilon \bigg \} \leq 2Ne^{-2n\epsilon^2} $$

Estos teoremas exhiben que utilizar el estimador de $L_n$ para elegir el $\phi$ dentro de una clase lleva a elegir funciones de decisión que minimizan $L$. Esto da fundamento teórico a los algoritmos de aprendizaje supervisado, en los cuales se utilizan métodos numéricos para buscar un $\phi$ que minimiza una estimación de la función de pérdida, la cual se calcula a partir de los datos de entrenamiento.
\section{Clase 6 (12/10)}
\subsection{Lema Borel-Cantelli}
Sea $A_n$ con $n=1,2,...$ una secuancia de ventos infinita.
Si se da que $$ \sum_{n=1}^{\infty} \mathds{P}(A_n) < \infty $$
Entonces: $$ \mathds{P}\big( \bigcap_{n=1}^{\infty} \bigcup_{m=n}^{\infty} A_m \big)=0 $$
\subsection{Teorema de Glivenko-Cantelli}
Sean $Z_1, ..., Z_n$ variables aleatorias identicamente distribuidas, con función de distribución $F$. Sea 
$$F_n(z)=\frac{1}{n} \sum_{i=1}^n \mathds{1} \{ Z_i \leq z \}$$
$$ \mathds{P} \bigg \{ \mathop{sup}_{z \in \mathds{R}} |F(z)-F_n(z)|>\epsilon \bigg \} \leq 8(n+1)e^{-n\epsilon^2/32}$$
Por el lema Borel-Cantelli:
$$ \mathop{lim}_{n \rightarrow \infty} \mathop{sup}_{z \in \mathds{R}} |F(z)-F_n(z)|=0 $$

La desigualdad de Vapnik-Chervonenkis puede comprenderse como una generalización de este teorema para cuando $z \in \mathds{R}^d$ (en este teorema se requiere $F$, lo cual implica que $z \in \mathds{R}$)

\subsection{Desigualdad Vapnik-Chervonenkis}
\subsubsection{Definiciones previas}
Sea $\mathds{A}$ un conjunto de conjuntos medibles. Sean $z_1, ...., z_n \in \mathds{R}^d$. Sea:
$$ N_{\mathds{A}}(z_1, ..., z_n)=\bigg | \big \{ \{ z_1, ..., z_n\} \cap A; \cap A \in \mathds{A} \big \} \bigg | $$

Sea entonces el N-avo coeficiente de destrozo/shattering/astillado de $\mathds{A}$:
$$ s(\mathds{A},n) = \mathop{max}_{(z_1,...,z_n) \in \{ \mathds{R}^d \}^n} N_{\mathds{A}}(z_1, ..., z_n)$$

$ s(\mathds{A},n)$ es la máxima cantidad de diferentes subconjuntos de n puntos que pueden ser ``pescados'' por la clase de conjuntos $\mathds{A}$. En cierta forma podría decirse que $ s(\mathds{A},n)$ es una medida de cuán ``versátil'', ``adaptable'' o ``expresiva'' $\mathds{A}$ es $\mathds{A}$.

Se definen Además

$$\nu(A)=\mathds{P}(Z_i \in A)$$
$$\nu_n(A)=\frac{1}{n} \sum_{i=1}^n \mathds{1} \{ Z_i \in A \}$$

Siendo $Z_i$ cualquier $Z_1, Z_2,... Z_n$ (son variables aleatorias en $ \mathds{R}^d $ identicamente distribuidas).

Entonces la desigualdad Vapnik-Chervonenkis indica que, para cualquier medida de probabilidad $\nu$, clase de conjuntos $\mathds{A}$, $n$ y $\epsilon>0$:

$$ \mathds{P} \big \{ \mathop{sup}_{A\in \mathds{A}} | \nu_n(A) - \nu(A) | > \epsilon \big \} \leq 8 s(\mathds{A},n)e^{-n\epsilon^2 / 32} $$

De esta desigualdad me llaman la atención dos cosas:
\begin{itemize}
  \item Fundamenta el fenómeno de "overfitting", ya que la distancia entre la estimación de $\nu$ y el $\nu$ real son más distantes cuando aumenta la capacidad expresiva de $\mathds{A}$
  \item Fundamenta el hecho de que el overfitting puede ser contrarrestado con más datos de entrenamiento, lo cual puede notarse en el hecho de que la cota decrece exponencialmente al crecer $n$.
\end{itemize}

\section{Clase 7 (19/10)}
esta es complicada creo
\end{document}