\documentclass[12pt, a4paper]{article}
\usepackage[utf8x]{inputenc}
\usepackage[spanish]{babel}
\usepackage{pdfpages}
\usepackage[T1]{fontenc} %Me deja combinar la negrita con las Mayusculas

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{dsfont} %para el 1
\usepackage{mathrsfs}
\usepackage{enumitem} %Para la enumeracion con letras

\usepackage{graphicx} %para la imagen
\usepackage{subcaption} %para poner varias imagenes juntas
\usepackage{float}


\usepackage{chngcntr} %Para resetear el contador de las ecuaciones por secciones y subsecciones
\counterwithin*{equation}{section}
\counterwithin*{equation}{subsection}

\title{Tarea de Aprendizaje Estadístico}
\author{}
\date{}

\begin{document}
\begin{titlepage} %Inicio de la caratula del tp
	\centering
	  \includegraphics[width=0.15\textwidth]{FIUBA_logo}\par
	  {\scshape\Large Universidad de Buenos Aires
      \\ Facultad de Ingenieria \par}
      {\scshape\small Año 2018 - 2er Cuatrimestre \par}
	  \vspace{1cm}
	  {\scshape\bfseries\LARGE Aprendizaje Estadístico, Teoría y aplicación\par}
	  \vspace{0.5cm}
	  \vspace{1cm}
      {\scshape\large Trabajo Práctico Final \par}
      \vspace{0.5cm
      \raggedright}
      \vspace{0.5cm}
    \centering
	  {\normalsize Sbruzzi, José Ignacio - Ingeniería Informática \#97452 \par}
      {\small  jose.sbru@gmail.com \par}
\end{titlepage} %Cerrado de la caratula del tp
\newpage
\tableofcontents
\newpage
\section{Reconstrucción de la prueba del teorema 5.2}
\subsection{Esquema de la prueba}

\begin{enumerate}
  \item Descomponer $ \mathds{E}[ (m_n(x) - m(x))^2 | X_1, ..., X_n] $ en dos términos
  \item Acotar el primer término aplicando esas definiciones
  \item Acotar el segundo término utilizando la propiedad de Lipschitz
  \item Utilizar la descomposición anterior para descomponer $\mathds{E}|| m_n -m ||^2$
  \item Acotar el primer término de la segunda descomposición
  \item Acotar el segundo término de la segunda descomposición
  \item Calcular la integral $\int_{S^{*}} \frac{1}{n\mu(S_{x,h_n})} \mu(dx) $
  \item Aplicar las cotas y el resultado de la integral en la segunda descomposición 
\end{enumerate}

\subsection{Prueba del teorema 5.2}
\subsubsection{Descomponer $ \mathds{E}[ (m_n(x) - m(x))^2 | X_1, ..., X_n] $ en dos términos}

$$
\begin{aligned}
\mathds{E}[ (m_n(x) - m(x))^2 | X_1, ..., X_n ] &= \\
&= \mathds{E}[ (m_n(x) - \hat{m_n}(x))^2 | X_1, ..., X_n ] + (\hat{m_n}(x) - m(x))^2 \\ 
&= A + B
\end{aligned}
$$

$$
\hat{m}_n(x) = \frac{ \sum_{i=1}^{n} m(X_i) \mathds{1}\{ X_i \in S_{x,h_n} \} }{n \mu_n(S_{x,h_n})}
$$

\subsubsection{Acotar el primer término aplicando esas definiciones}

Sean:
$$
B_n(x)=\{ n \mu_n(S_x,h_n) > 0 \}
$$

$$
m_n(x)=\frac{ \sum_{i=1}^{n} Y_i \mathds{1}\{ X_i \in S_{x,h_n} \} }{n \mu_n(S_{x,h_n})}
$$

$$
m(x)=\mathds{E}[Y|X=x]
$$
$$
\mu_n(A)=\frac{1}{n}\sum_{i=1}^n \mathds{1}\{ X_i \in A \}
$$

Entonces, aplicando la segunda definición:
$$
\begin{aligned}
A &= \mathds{E}[ (m_n(x) - \hat{m_n}(x))^2 | X_1, ..., X_n ] \\
&= \mathds{E}\Bigg[ \Bigg( \frac{ \sum_{i=1}^{n} (Y_i - m(X_i)) \mathds{1}\{ X_i \in S_{x,h_n} \} }{n \mu_n(S_{x,h_n})} \Bigg)^2 | X_1, ..., X_n \Bigg] \\
&= \mathds{E}\Bigg[ \Bigg( \frac{ \sum_{i=1}^{n} (Y_i - \mathds{E}[Y|X=X_i]) \mathds{1}\{ X_i \in S_{x,h_n} \} }{n \mu_n(S_{x,h_n})} \Bigg)^2 | X_1, ..., X_n \Bigg] \\
&= \mathds{E}\Bigg[  \frac{ \sum_{i=1}^{n} (Y_i - \mathds{E}[Y|X=X_i])^2 \mathds{1}\{ X_i \in S_{x,h_n} \} }{(n \mu_n(S_{x,h_n}))^2}  | X_1, ..., X_n \Bigg] \\
&= \frac{ \sum_{i=1}^{n} \mathds{E}[(Y_i - \mathds{E}[Y|X=X_i])^2] \mathds{1}\{ X_i \in S_{x,h_n} \} }{(n \mu_n(S_{x,h_n}))^2} \\
&= \frac{ \sum_{i=1}^{n} \mathop{Var}(Y_i|X_i) \mathds{1}\{ X_i \in S_{x,h_n} \} }{(n \mu_n(S_{x,h_n}))^2} \\
&= \mathop{Var}(Y|X) \frac{ \sum_{i=1}^{n} \mathds{1}\{ X_i \in S_{x,h_n} \} }{(n \mu_n(S_{x,h_n}))^2} \\
&= \mathop{Var}(Y|X) \frac{n \mu_n(S_{x,h_n}) }{(n \mu_n(S_{x,h_n}))^2} \\
&= \mathop{Var}(Y|X) \frac{ 1 }{n \mu_n(S_{x,h_n})} \mathds{1} \{ B_n(x)\}\\
&\leq \sigma^2 \frac{ 1 }{n \mu_n(S_{x,h_n})} \mathds{1} \{ B_n(x)\}\\
\end{aligned}
$$
Aclaración: el $\mathds{1} \{ B_n(x)\}$ final se agrega para que la última expresión pueda valer 0 en el caso de que para ningún $i$ se cumpla $X_i \in S_{x,h_n}$, de esta forma se preserva la igualdad en ese caso.

Nota: en la demostración se asume que un único $X_i$ puede pertenecer a un $S_{x,h_n}$, de esta forma se puede operar con el cuadrado tal como se hace en el desarrollo arriba (página 78 gyorfi).

\subsubsection{Acotar el segundo término utilizando la propiedad de Lipschitz}
La constante de Lipschitz $C$ es la menor $c \in \mathds{R}$ que cumple 
$$
| f(x_1) -f(x_2) | \leq c | x_1 - x_2 |
$$
Así, $ |m(X_i)-m(x)| \leq C h_n $ porque $|X_i - x| \leq h_n$.

Con lo cual, operando con el cuadrado tal como en la sección anterior, tenemos (página 78):

$$
\begin{aligned}
  B 
  &= (\hat{m}_n(x)-m(x))^2 \\
  &=  \Bigg( \frac{ \sum_{i=1}^{n} (m(X_i) - m(x)) \mathds{1}\{ X_i \in S_{x,h_n} \} }{n \mu_n(S_{x,h_n})} \Bigg)^2 \mathds{1}\{ B_n(x) \} + m(x)^2 \mathds{1}\{ \text{ no } B_n(x) \} \\
  &=   \frac{ \sum_{i=1}^{n} (m(X_i) - m(x))^2 \mathds{1}\{ X_i \in S_{x,h_n} \} }{n \mu_n(S_{x,h_n})}  \mathds{1}\{ B_n(x) \} + m(x)^2 \mathds{1}\{ \text{ no } B_n(x) \} \\
  &\leq   \frac{ \sum_{i=1}^{n} (C h_n)^2 \mathds{1}\{ X_i \in S_{x,h_n} \} }{n \mu_n(S_{x,h_n})}  \mathds{1}\{ B_n(x) \} + m(x)^2 \mathds{1}\{ \text{ no } B_n(x) \} \\
  &= (C h_n)^2  \frac{ \sum_{i=1}^{n}  \mathds{1}\{ X_i \in S_{x,h_n} \} }{n \mu_n(S_{x,h_n})}  \mathds{1}\{ B_n(x) \} + m(x)^2 \mathds{1}\{ \text{ no } B_n(x) \} \\
  &= (C h_n)^2    \mathds{1}\{ B_n(x) \} + m(x)^2 \mathds{1}\{ \text{ no } B_n(x) \} \\
  &\leq (C h_n)^2 + m(x)^2 \mathds{1}\{ \text{ no } B_n(x) \}
\end{aligned}
$$

Nota: la primera descomposición en una suma se debe a que se prserva el valor de la igualdad cuando se cumple $B_n(x)$ y cuando no se cumple.

\subsubsection{Utilizar la descomposición anterior para descomponer $\mathds{E}|| m_n -m ||^2$}

$$
\begin{aligned}
  \mathds{E}|| m_n -m ||^2 \\
  &= \mathds{E} \Bigg\{ \int (m_n(x)-m(x))^2 \mu(dx) 
  \Bigg\} \\
  &= \mathds{E} \Bigg\{ \int_{S^{*}} (A+B) \mu(dx) 
  \Bigg\} \\
  &= \mathds{E} \Bigg\{ \int_{S^{*}} A \mu(dx) 
  \Bigg\} + \mathds{E} \Bigg\{ \int_{S^{*}} B \mu(dx) 
  \Bigg\} \\
  &= \mathds{E} \Bigg\{ \int_{S^{*}} 
      \sigma^2 \frac{ 1 }{n \mu_n(S_{x,h_n})} \mathds{1} \{ B_n(x)\}
   \mu(dx) \Bigg\} 
    + \\ &+ 
   \mathds{E} \Bigg\{ \int_{S^{*}} 
    (C h_n)^2 + m(x)^2 \mathds{1}\{ \text{ no } B_n(x) \}
   \mu(dx) 
  \Bigg\} \\
  &= A' + B'
\end{aligned}
$$

\subsubsection{Acotar el primer término de la segunda descomposición}
Por la definición de $\mu_n(A)$ y la de $B_n(x)$
$$
\begin{aligned}
  A' &= \\
  &=\mathds{E} \Bigg\{ \int_{S^{*}} \sigma^2 \frac{ 1 }{n \mu_n(S_{x,h_n})} \mathds{1} \{ B_n(x)\} \mu(dx) \Bigg\} \\
  &=\sigma^2 \mathds{E} \Bigg\{ \int_{S^{*}}  \frac{ \mathds{1} \{ B_n(x)\} }{n (\frac{1}{n} \sum_{i=1}^n \mathds{1}\{ X_i \in S_{x,h_n} \})}  \mu(dx) \Bigg\} \\
  &=\sigma^2 \mathds{E} \Bigg\{ \int_{S^{*}}  \frac{ \mathds{1} \{ 
    \sum_{i=1}^n \mathds{1}\{ X_i \in S_{x,h_n} \}>0  
  \} }{ \sum_{i=1}^n \mathds{1}\{ X_i \in S_{x,h_n} \}}  \mu(dx) \Bigg\}
\end{aligned}
$$

$\sum_{i=1}^n \mathds{1}\{ X_i \in S_{x,h_n} \}$ es una variable aleatoria con una distribución binomial de parámetros $n=n$ y $p=\mathds{P}( X \in S_{x,h_n} )=\mu(S_{x,h_n})$ por la definición de $\mu(A)$.

El lema 4.1 del libro indica que, siendo $B(n,p)$ una variable aleatoria de distribución binomial con parámetros $n$ y $p$, vale que 
$$
\mathds{E}[\frac{1}{B(n,p)} \mathds{1} \{ B(n,p)>0 \}] \leq \frac{2}{(n+1)p} \leq \frac{2}{np}
$$

Aplicando este lema:

$$
\begin{aligned}
  A' &= \\
  &= \sigma^2 \int_{S^{*}} \mathds{E} \Bigg\{   \frac{ \mathds{1} \{ 
    \sum_{i=1}^n \mathds{1}\{ X_i \in S_{x,h_n} \}>0  
  \} }{ \sum_{i=1}^n \mathds{1}\{ X_i \in S_{x,h_n} \}} \Bigg\} \mu(dx) \\
  &\leq \sigma^2 \int_{S^{*}} \frac{2}{n\mu(S_{x,h_n})}\mu(dx) \\
  &= 2 \sigma^2 \int_{S^{*}} \frac{1}{n\mu(S_{x,h_n})}\mu(dx) \\
\end{aligned}
$$

\subsubsection{Acotar el segundo término de la segunda descomposición}

$$
\begin{aligned}
  B' &= \\
  &= \mathds{E} \Bigg\{ \int_{S^{*}} \Big(
    (C h_n)^2 + m(x)^2 \mathds{1}\{ \text{ no } B_n(x) \}
    \Big) \mu(dx) 
  \Bigg\} \\
  &= \mathds{E} \Bigg\{ \int_{S^{*}} \Big(
     m(x)^2 \mathds{1}\{ \text{ no } B_n(x) \}
    \Big) \mu(dx) 
    +
    \int_{S^{*}} (C h_n)^2 \mu(dx) 
  \Bigg\} \\
  &= \mathds{E} \Bigg\{ \int_{S^{*}} \Big(
     m(x)^2 \mathds{1}\{ \text{ no } B_n(x) \}
    \Big) \mu(dx) 
    +
    (C h_n)^2 \int_{S^{*}} \mu(dx) 
  \Bigg\} \\
  &= \mathds{E} \Bigg\{ \int_{S^{*}} \Big(
     m(x)^2 \mathds{1}\{ \text{ no } B_n(x) \}
    \Big) \mu(dx) 
    \Bigg\}
    +
    \mathds{E} \Bigg\{
      (C h_n)^2 \int_{S^{*}} \mu(dx) 
    \Bigg\} \\
  &= \int_{S^{*}} \mathds{E} \bigg\{ 
     m(x)^2 \mathds{1}\{ \text{ no } B_n(x) \}
    \bigg\}
    \mu(dx) 
    +
    \mathds{E} \Bigg\{
      (C h_n)^2 \cdot 1
    \Bigg\} \\
    &= \int_{S^{*}} m(x)^2 \mathds{E} \bigg\{ 
      \mathds{1}\{ \text{ no } B_n(x) \}
    \bigg\}
    \mu(dx) 
    + (C h_n)^2
\end{aligned}
$$

A continuación se analiza $\mathds{E} \bigg\{ 
  \mathds{1}\{ \text{ no } B_n(x) \}
\bigg\}$ :
$$
\begin{aligned}
\mathds{E} \bigg\{ 
      \mathds{1}\{ \text{ no } B_n(x) \}
    \bigg\}&= \\
&= \mathds{E} \bigg\{ 
  \mathds{1}\{
    \mu_n(S_{x,h_n})=0
  \}\\
&= \mathds{P} \bigg\{ 
  \mu_n(S_{x,h_n})=0
\bigg\} \cdot 1 
+ 
\mathds{P} \bigg\{ 
    \mu_n(S_{x,h_n})>0
\bigg\} \cdot 0 \\
&= \mathds{P} \bigg\{ 
  \sum_{i=1}^n \mathds{1} \{ X_i \in S_{x,h_n} \} =0
\bigg\} \\
&= \mathds{P} \bigg\{ 
  Binomial(n,\mu(S_{x,h_n}))=0
\bigg\} \\
&= (1-\mu(S_{x,h_n}))^n
\end{aligned}
$$

Con lo cual
$$
\begin{aligned}
B' &= \\
&= \int_{S^{*}} m(x)^2 \mathds{E} \bigg\{ 
  \mathds{1}\{ \text{ no } B_n(x) \}
\bigg\}
\mu(dx) 
+ (C h_n)^2 \\
&= \int_{S^{*}} m(x)^2 (1-\mu(S_{x,h_n}))^n \mu(dx) 
+ (C h_n)^2
\end{aligned}
$$

A continuación se acota la integral:
$$
\begin{aligned}
B' &= \\
&\leq(C h_n)^2 + \mathop{sup}_{z \in S^{*}} m(z)^2 \int_{S^{*}} e^{-n\mu(S_{x,h_n})} \mu(dx) \\
&\leq(C h_n)^2 + \mathop{sup}_{z \in S^{*}} m(z)^2 \mathop{max}_u ue^{-u} \int_{S^{*}} \frac{1}{n\mu(S_{x,h_n})} \mu(dx) \\
\end{aligned}
$$

\subsubsection{Calcular la integral $\int_{S^{*}} \frac{1}{n\mu(S_{x,h_n})} \mu(dx) $}
En el libro se indica que tal integral se calcula de la siguiente forma, admitiendo $S\subset S^{*}$ (página 76):

Se eligen $z_1, ..., z_{M_n}$ tales que la unión de $S_{z_1, h_n/2}, ..., S_{z_{M_n}, h_n/2}$ cubre $S$, y $$ M_n \leq \frac{\tilde{c}}{h_n^d} $$
Siendo $\tilde{c}$ una constante.

Inicialmente se acota la integral sobre $S$ con la suma de las integrales sobre los $S_{z_i,h_n/2}$
$$
\begin{aligned}
\int_{S} \frac{1}{n\mu(S_{x,h_n})} \mu(dx)  &\leq \\
&\leq \sum_{j=1}^{M_n} \int_{S} \frac{\mathds{1}\{ x\in S_{z_j,h_n/2} \}}{n\mu(S_{x,h_n})} \mu(dx) \\
&= \sum_{j=1}^{M_n} \int_{S_{z_j,h_n/2}} \frac{1}{n\mu(S_{x,h_n})} \mu(dx) \\
&\leq \sum_{j=1}^{M_n} \int_{S_{z_j,h_n/2}} \frac{1}{n\mu(S_{z_j,h_n/2})} \mu(dx) \\
&= \sum_{j=1}^{M_n} \frac{1}{n\mu(S_{z_j,h_n/2})} \int_{S_{z_j,h_n/2}}  \mu(dx) \\
\text{Por la definición de $\mu$:} \\
&= \sum_{j=1}^{M_n} \frac{1}{n\mu(S_{z_j,h_n/2})}  \mu(S_{z_j,h_n/2}) \\
&= \sum_{j=1}^{M_n} \frac{1}{n} \\
&= \frac{M_n}{n} \\
&\leq \frac{\tilde{c}}{nh_n^d} \\
\end{aligned}
$$
\subsubsection{Aplicar las cotas y el resultado de la integral en la segunda descomposición}
$$
\begin{aligned}
A' + B' &\leq \\
&\leq 2 \sigma^2 \int_{S^{*}} \frac{1}{n\mu(S_{x,h_n})}\mu(dx) +\\&+
(C h_n)^2 
+\\&+
\mathop{sup}_{z \in S^{*}} m(z)^2 \mathop{max}_u ue^{-u} \int_{S^{*}} \frac{1}{n\mu(S_{x,h_n})} \mu(dx) \\
&\leq 2 \sigma^2\frac{\tilde{c}}{nh_n^d} +
(C h_n)^2 
+
\mathop{sup}_{z \in S^{*}} m(z)^2 \mathop{max}_u ue^{-u} \frac{\tilde{c}}{nh_n^d} \\
\end{aligned}
$$
Debido a que $ue^{-u}$ es máximo cuando $u=1$:
$$
\begin{aligned}
A' + B' &\leq \\
&\leq 2 \sigma^2 \frac{\tilde{c}}{nh_n^d} +
(C h_n)^2 
+
\mathop{sup}_{z \in S^{*}} m(z)^2 \frac{\tilde{c}}{nh_n^d} \\
&= \frac{\tilde{c}}{nh_n^d} (2 \sigma^2 +
\mathop{sup}_{z \in S^{*}} m(z)^2)  + (C h_n)^2 \\
&= \tilde{c} \frac{2 \sigma^2 +
\mathop{sup}_{z \in S^{*}} m(z)^2}{nh_n^d}  + (C h_n)^2 \\
&\leq \hat{c} \frac{\sigma^2 +
\mathop{sup}_{z \in S^{*}} m(z)^2}{nh_n^d}  + (C h_n)^2 \\
\end{aligned}
$$

Donde $\hat{c}=2\tilde{c}$ 

\subsubsection{Análisis de la segunda proposición del teorema}
Al reemplazar el $h_n$ propuesto en la expresión final alcanzada, se obtiene la segunda proposición del teorema, en la que $c''=c'+\hat{c}(c')^{-d}=c'+2\tilde{c}(c')^{-d}$.

\section{Idea - introducción teórica}
El objetivo es comprobar empíricamente el teorema 5.2 del Gyorfy:

\begin{quotation}
\textbf{Theorem  5.2:} For a kernel estimate with a naive kernel assume that

$$
\mathop{Var}(Y|X=x)\leq \sigma^2 ,x \in \mathds{R}^d
$$

and
$$
|m(x)-m(z)|\leq||x-z||, x,z \in \mathds{R}^d
$$

and $X$ has a compact support $S^{*}$. Then

$$
\mathds{E}|| m_n -m ||^2 \leq \hat{c} \dfrac{ \sigma^2 + \mathop{sup}_{z\in S^{*}} |m(z)|^2 }{n \cdot h_n^d} +C^2h_n^2
$$
where $\hat{c}$ depends only on the diameter of $S^{*}$ and on $d$, thus for

$$
h_n = c' \Bigg( \dfrac{\sigma^2 + \mathop{sup}_{z\in S^{*}}|m(z)|^2 }{C^2} \Bigg)^{1/(d+2)} n^{\Big (-\dfrac{1}{d+2}\Big )}
$$

we have

$$
\mathds{E}|| m_n - m ||^2 \leq c'' \Bigg (  \sigma^2 + \mathop{sup}_{z\in S^{*}} |m(z)|^2 \Bigg )^{2/(d+2)} C^{2d/(d+2)} n^{-2/(d+2)}
$$

\end{quotation}

Para esto, se acota el problema a la siguiente situación:
\begin{itemize}
  \item $ x_i \in D$
  \item $|m(z)|\leq 1 $ para todo $z\in D$
  \item El ruido agregado a $m(z)$ para generar los pares $x_i$, $y_i$ es una normal estándard, con lo cual $\mathop{Var}(Y|X=x) = 1$, es decir, $\sigma=1$
\end{itemize}

Siendo $D=[-1,1]^d$

Así, queda acotado también $C$. De esta forma, la última ecuación del teorema puede escribirse como:
$$
\mathds{E}||m_n - m||^2 \leq c'' (1+1)^{2/(d+2)} C^{2d/(d+2)} n^{-d/(d+2)}
$$
Podemos hacer algo similar con la primera conclusión del teorema:
$$
\mathds{E}|| m_n -m ||^2 \leq \hat{c} \dfrac{ 1 + 1 }{n \cdot h_n^d} +C^2h_n^2
$$
\section{Primera estrategia}

\subsection{Estimación de $\mathds{E}|| m_n-m ||^2$}
A continuación se explican los pasos que usa el programa para estimar este valor para determinados $n$, $d$ y $h_n$.
\begin{enumerate}
  \item generar una función $m(\cdot)$ con $-1 \leq m(x) \leq 1$ para todo $x \in D$.
  \item generar una función $s(x)$ con las mismas características
  \item Generar un conjunto $P$ de $n$ pares $(x_i,y_i)$ tales que $y_i = m(x_i) + S$, donde $S$ tiene una distribución normal centrada en 0 y con una varianza $|s(x_i)| \leq 1$. Los puntos $x_i$ pertenecen a $D$, es decir, tienen $d$ dimensiones.
  \item A partir de este conjunto $P$ de pares, se genera una estimación de la regresión, $m_n$, usando un naive kernel y el $h_n$ correspondiente.
  \item Teniendo $m(x)$ y $m_n(x)$ definidos para todo $x\in D$, se utiliza la librería de python mcint para integrar $(m(x)-m_n(x))^2$ sobre todo $D$. mcint utiliza técnicas montecarlo para estimar la integral, ya que para $d$ dimensiones la integral es difícil de calcular numericamente (es decir, tarda demasiado). Así se obtiene un $||m-m_n||^2$.
  \item Se repite este procedimiento para una cantidad de $m(\cdot)$, $s(\cdot)$ y $m_n(\cdot)$ generadas al azar (en la mayoría de los casos se hicieron 300 experimentos para cada $n$ y $d$, en otros casos se hicieron 100).
  \item Se promedian los $||m-m_n||^2$ para obtener una estimación de la esperanza.
\end{enumerate}
Así, se obtiene la función $\mathop{encontrarEError}(n,d,h_n)$.

\subsection{Verificación del teorema}
La idea inicial era verificar que al variar $n$ y mantener fijo $d$ y $h_n$, se cumpliría que existe una cota de la forma $$c(n^{-k})$$ que cumpla:
\begin{itemize}
  \item $c(n^{-k})$ es mayor que todas las estimaciones $\mathop{encontrarEError}(n)$ ($d$ y $h_n$ son fijos)
  \item Los $c$ y $k$ elegidos deben ser tales que minimicen $\sum_{i=1}^n c(n^{-k})$
\end{itemize}
Así, la curva más ajustada a los datos (es decir, con $c$ y $k$ mejores que los que propone el teorema), debería cumplir que $k$ sea mejor al propuesto por el teorema (el teorema indica $k=2/(d+2)$) para verificarlo.

También se analizó la curva que cumple $k=2/(d+2)$. En este caso simplemente se agregó esta condición sobre $k$ y se buscó sólo el $c$ que cumpla las condiiciones listadas arriba.

Esta prueba se repitió para $h_n=0.5$ y $h_n=0.1$.

\includegraphics[width=\textwidth]{figuras_h=0.1/cotas-error-d=1}

\section{Primeros resultados: $h_n=0.1$}

\includegraphics[width=0.5\textwidth]{figuras_h=0.1/cotas-error-d=1}
\includegraphics[width=0.5\textwidth]{figuras_h=0.1/cotas-error-d=2}
\includegraphics[width=0.5\textwidth]{figuras_h=0.1/cotas-error-d=3}
\includegraphics[width=0.5\textwidth]{figuras_h=0.1/cotas-error-d=4}
\includegraphics[width=0.5\textwidth]{figuras_h=0.1/cotas-error-d=6}
\includegraphics[width=0.5\textwidth]{figuras_h=0.1/cotas-error-d=8}
\includegraphics[width=0.5\textwidth]{figuras_h=0.1/cotas-error-d=10}
\includegraphics[width=0.5\textwidth]{figuras_h=0.1/cotas-error-d=30}

\includegraphics[width=\textwidth]{figuras_h=0.1/k-variando-d}

\includegraphics[width=\textwidth]{figuras_h=0.1/resultados-grales}


\section{Primeros resultados: $h_n=0.5$}

\includegraphics[width=0.5\textwidth]{figuras_h=0.5/cotas-error-d=1}
\includegraphics[width=0.5\textwidth]{figuras_h=0.5/cotas-error-d=2}
\includegraphics[width=0.5\textwidth]{figuras_h=0.5/cotas-error-d=3}
\includegraphics[width=0.5\textwidth]{figuras_h=0.5/cotas-error-d=4}
\includegraphics[width=0.5\textwidth]{figuras_h=0.5/cotas-error-d=6}
\includegraphics[width=0.5\textwidth]{figuras_h=0.5/cotas-error-d=8}
\includegraphics[width=0.5\textwidth]{figuras_h=0.5/cotas-error-d=10}

\includegraphics[width=\textwidth]{figuras_h=0.5/k-variando-d}

\includegraphics[width=\textwidth]{figuras_h=0.5/resultados-grales}

\section{Conclusiones para $h_n$ constante}
Para $h_n=0.1$ se obtuvieron resultados buenos (que verifican) para $d=1$, $d=2$ y $d=3$, pero para $d$ superiores no se logró la verificación. 

Esto es razonable cuando se tienen en cuenta las reglas usadas en la práctica "común" de machine learning: al aumentar la cantidad de dimensiones y no subsanar esto con más datos se tiene underfitting. 

En este caso en particular también es importante la maldición de la dimensionalidad: a medida que crece $d$, la cantidad de puntos a una distancia fija $h_n$ cae (en realidad, el mismísimo significado de la distancia es el que se pierde: todos los puntos tienden a estar a una distancia muy similar unos de otros).

Aunque las "reglas prácticas del machine learning" y la maldición de la dimensionalidad explican los resultados, estos contradicen al teorema que se busca corroborar empiricamente (es decir, la interpretación que se había hecho del mismo).

Con el objetivo de observar el comportamiento del algoritmo en dimensiones altas, se utilizó $h_n=0.5$ para realizar una nueva prueba.

Los resultados de esta prueba son muy importantes: para dimensiones "medianas"  ( $d=4$, $d=6$), con $h_n=0.5$ se logra lo que no se puede con $h_n=0.1$: se tiene una mejora gradual y se corrobora el teorema para $d=6$.

El problema es que fue ignorada la condición sobre $h_n$ que requiere la segunda conclusión del teorema:


$$
h_n = c' \Bigg( \dfrac{\sigma^2 + \mathop{sup}_{z\in S^{*}}|m(z)|^2 }{C^2} \Bigg)^{1/(d+2)} n^{\Big (-\dfrac{1}{d+2}\Big )}
$$

\section{Segunda estrategia}

Es imposible fijar 
$$
h_n = c' \Bigg( \dfrac{\sigma^2 + \mathop{sup}_{z\in S^{*}}|m(z)|^2 }{C^2} \Bigg)^{1/(d+2)} n^{\Big (-\dfrac{1}{d+2}\Big )}
$$
ya que para eso sería necesario conocer $c'$.

Entonces se intenta corroborar el teorema 5.1, que establece:

\begin{quotation}
\textbf{Theorem 5.1:} Assume that there are balls $S_{0,r}$ of radius $r$ and balls $S_{0,R}$
of radius $R$ centered at the origin ($0 < r\leq R$), and constant $b>0$ such that
$$
\mathds{1}\{x\in S_{0,R}\}\geq K(x) \geq b \mathds{1}\{x\in S_{0,r}\}
$$
(boxed kernel), and consider the kernel estimate $m_n$ if $h_n \rightarrow 0$ and $n h_n^d \rightarrow \infty$, then the kernel estimate is weakly universally consistent.
\end{quotation}

El kernel naive es un boxed kernel, por lo tanto, si se cumplen las condiciones sobre $h_n$ que establece este teorema, se obtiene la consistencia debil.

Así, se llevaron adelante dos pruebas: una con $h_n$ dependiendo de $n$ y $d$, y otra en la cual depende sólo de $d$. Así, para la primera, se usó $$ 0.8548 (n^{(-1/(4.054 \cdot d))}) $$, lo cual cumple las condiciones y además cumple que $h_n$ es aproximadamente $0.1$ cuando $d=1$ y $n=8000$, y aproximadamente $0.5$ cuando $d=4$ y $n=8000$. Para la segunda corrida de pruebas se utilizó $h_n=10^{-1/d}$, elegido con el mismo criterio.

\section{Resultados para $h_n$ variando con $d$ y $n$}

\includegraphics[width=0.5\textwidth]{figuras_h_dinamico/cotas-error-d=1}
\includegraphics[width=0.5\textwidth]{figuras_h_dinamico/cotas-error-d=2}
\includegraphics[width=0.5\textwidth]{figuras_h_dinamico/cotas-error-d=3}
\includegraphics[width=0.5\textwidth]{figuras_h_dinamico/cotas-error-d=4}
\includegraphics[width=0.5\textwidth]{figuras_h_dinamico/cotas-error-d=6}
\includegraphics[width=0.5\textwidth]{figuras_h_dinamico/cotas-error-d=8}
\includegraphics[width=0.5\textwidth]{figuras_h_dinamico/cotas-error-d=10}

\includegraphics[width=\textwidth]{figuras_h_dinamico/k-variando-d}

\includegraphics[width=\textwidth]{figuras_h_dinamico/resultados-grales}


\section{Resultados para $h_n$ variando sólo con $d$}

\includegraphics[width=0.5\textwidth]{figuras_h_semidinamico/cotas-error-d=1}
\includegraphics[width=0.5\textwidth]{figuras_h_semidinamico/cotas-error-d=2}
\includegraphics[width=0.5\textwidth]{figuras_h_semidinamico/cotas-error-d=3}
\includegraphics[width=0.5\textwidth]{figuras_h_semidinamico/cotas-error-d=4}
\includegraphics[width=0.5\textwidth]{figuras_h_semidinamico/cotas-error-d=6}
\includegraphics[width=0.5\textwidth]{figuras_h_semidinamico/cotas-error-d=8}
\includegraphics[width=0.5\textwidth]{figuras_h_semidinamico/cotas-error-d=10}

\includegraphics[width=\textwidth]{figuras_h_semidinamico/k-variando-d}

\includegraphics[width=\textwidth]{figuras_h_semidinamico/resultados-grales}

\section{Conclusiones para $h_n$ variable}

En caso de $h_n$ variable, se obtuvieron mejores resultados para todas las curvas: hubo aprendizaje para $d$ entre 1 y 6, lo cual era de esperar debido al criterio utilizado para elegir las constantes, discutido previamente; y también hubo aprendizaje para $d=8$, lo cual no se había observado para $h_n=0.1$ ni $h_n=0.5$. Esto es un indício fuerte para opinar que es posible corroborar el teorema 5.1.

El aprendizaje para $d=8$ es muy notorio para $h_n$ dependiente sólo de $d$, y menos notorio para $h_n$ dependiente de $n$, lo cual podría resultar contradictorio al teorema.

El hecho de que para $d=10$ no se dé el aprendizaje podría deberse a dos motivos:
\begin{enumerate}
  \item Las constantes elegidas para las funciones $h_n(n,d)$ y $h_n(d)$ son incorrectas.
  \item Las formas generales elegidas para $h_n(n,d)$ y $h_n(d)$ son incorrectas.
\end{enumerate}

Otra posibilidad es que el teorema sea imposible de corroborar en altas dimensiones debido a la maldición de la dimensionalidad: que el aprendizaje sea demasiado lento como para observarlo.



\end{document}
